{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9acb59df",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ee909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "csv_path = ( \"input_file_path.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "FEATURES = ['Ti','Er','Rb','Ho','Pu','Cs','Zr','S','Tb','N','Ge','Pd','Mg','Re','La','K',\n",
    "    'Hf','P','Br','Ag','Os','F','Sc','Cm','Mo','In','Cl','Hg','Se','Tm','Ir','W',\n",
    "    'Th','H','Te','Np','Zn','Li','Gd','Ni','Co','Bi','I','Pr','Cd','Nb','Pa','Pt',\n",
    "    'Si','U','V','Sb','Mn','Na','Ce','Yb','Ta','Nd','Rh','O','Au','Sr','Eu','C',\n",
    "    'Pb','Ca','Cr','Cu','Ga','Fe','Y','As','Sn','B','Ba','Dy','Be','Sm','Lu','Al',\n",
    "    'Tl','Ru','Avg_Atomic_Number','Average_Weight','Average_Electronegativity',\n",
    "    'Magnetic_proportion','Entropy','average_period','avg_magnetic_moment',\n",
    "    'average_group','Rare_Earth_proportion'\n",
    "]\n",
    "TARGET = 'Mean_TC_K'\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "bins = pd.cut(\n",
    "    y,\n",
    "    bins=[-np.inf,  60, 165, 270, 325, 500, 665, np.inf],\n",
    "    labels=False\n",
    ")\n",
    "\n",
    "# Stratified train/test split\n",
    "X_train, X_test, y_train, y_test, bins_train, bins_test = train_test_split(\n",
    "    X, y, bins,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=bins\n",
    ")\n",
    "\n",
    "\n",
    "train_bin_counts = pd.Series(bins_train).value_counts().sort_index()\n",
    "print(\"Train bin counts:\\n\", train_bin_counts)\n",
    "minority_bin_size = train_bin_counts.min()\n",
    "print(f\"\\nSmallest bin size = {minority_bin_size} samples\\n\")\n",
    "\n",
    "\n",
    "best_params = {\n",
    "    'n_estimators':     1200,\n",
    "    'max_depth':        12,\n",
    "    'learning_rate':    0.08,\n",
    "    'subsample':        0.8,\n",
    "    'colsample_bytree': 0.6\n",
    "}\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "r2_scores, mae_scores = [], []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(cv.split(X_train, bins_train), 1):\n",
    "    Xt, Xv = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "    yt, yv = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    bins_tr = bins_train.iloc[tr_idx]  \n",
    "\n",
    "    \n",
    "    fold_counts = pd.Series(bins_tr).value_counts()\n",
    "    fold_min = fold_counts.min()\n",
    "    \n",
    "\n",
    "    \n",
    "    M = 5\n",
    "    val_preds = np.zeros((len(Xv), M))\n",
    "    for i in range(M):\n",
    "        idxs = []\n",
    "        for b in fold_counts.index:\n",
    "            b_idxs = np.where(bins_tr == b)[0]\n",
    "            if len(b_idxs) <= fold_min:\n",
    "                sel = b_idxs\n",
    "            else:\n",
    "                sel = np.random.RandomState(100+i).choice(\n",
    "                    b_idxs, size=fold_min, replace=False\n",
    "                )\n",
    "            idxs.append(sel)\n",
    "        idxs = np.concatenate(idxs)\n",
    "\n",
    "        mb = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=100+i,\n",
    "            n_jobs=1,\n",
    "            **best_params\n",
    "        )\n",
    "        mb.fit(Xt.iloc[idxs], yt.iloc[idxs])\n",
    "        val_preds[:, i] = mb.predict(Xv)\n",
    "\n",
    "    yv_pred = val_preds.mean(axis=1)\n",
    "    r2_scores.append(r2_score(yv, yv_pred))\n",
    "    mae_scores.append(mean_absolute_error(yv, yv_pred))\n",
    "    print(f\" Fold {fold:>2}:   R²={r2_scores[-1]:.3f}, MAE={mae_scores[-1]:.1f}\")\n",
    "\n",
    "print(\"\\nValidation performance (mean ± std):\")\n",
    "print(f\" R²  : {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
    "print(f\" MAE : {np.mean(mae_scores):.1f} ± {np.std(mae_scores):.1f}\\n\")\n",
    "\n",
    "\n",
    "M_final = 20\n",
    "models = []\n",
    "for i in range(M_final):\n",
    "    idxs = []\n",
    "    for b in train_bin_counts.index:\n",
    "        b_idxs = np.where(bins_train == b)[0]\n",
    "        if len(b_idxs) <= minority_bin_size:\n",
    "            sel = b_idxs\n",
    "        else:\n",
    "            sel = np.random.RandomState(200+i).choice(\n",
    "                b_idxs, size=minority_bin_size, replace=False\n",
    "            )\n",
    "        idxs.append(sel)\n",
    "    idxs = np.concatenate(idxs)\n",
    "\n",
    "    m = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=200+i,\n",
    "        n_jobs=1,\n",
    "        **best_params\n",
    "    )\n",
    "    m.fit(X_train.iloc[idxs], y_train.iloc[idxs])\n",
    "    models.append(m)\n",
    "\n",
    "\n",
    "all_preds = np.column_stack([m.predict(X_test) for m in models])\n",
    "y_pred = all_preds.mean(axis=1)\n",
    "y_std  = all_preds.std(axis=1)\n",
    "\n",
    "\n",
    "print(\" Test performance:\")\n",
    "print(\" R²  :\", r2_score(y_test, y_pred))\n",
    "print(\" MAE :\", mean_absolute_error(y_test, y_pred))\n",
    "print(\" RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.errorbar(\n",
    "    y_test, y_pred, \n",
    "    yerr=y_std, \n",
    "    fmt='o', \n",
    "    ecolor='lightgray', \n",
    "    capsize=2, \n",
    "    alpha=0.7\n",
    ")\n",
    "plt.plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    'r--', lw=1\n",
    ")\n",
    "plt.xlabel('Actual Mean Curie Temperature (K)')\n",
    "plt.ylabel('Predicted Mean Curie Temperature (K)')\n",
    "plt.title('Ensemble Predictions ±1σ on TEST')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cef3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b9d2ee",
   "metadata": {},
   "source": [
    "###Plot for the Predicted Vs Actual Temperature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b100f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"xtick.major.width\": 1.1,\n",
    "    \"ytick.major.width\": 1.1,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\"\n",
    "})\n",
    "\n",
    "\n",
    "norm = plt.Normalize(vmin=y_std.min(), vmax=y_std.max())\n",
    "colors = cm.turbo(norm(y_std))  \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.1, 6))  \n",
    "\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    y_test, y_pred,\n",
    "    c=y_std, cmap='turbo',\n",
    "    edgecolor='black', linewidth=0.25,\n",
    "    s=65, alpha=0.9\n",
    ")\n",
    "\n",
    "\n",
    "ax.errorbar(\n",
    "    y_test, y_pred,\n",
    "    yerr=y_std,\n",
    "    fmt='none',\n",
    "    ecolor='gray',\n",
    "    alpha=0.2,\n",
    "    capsize=2,\n",
    "    linewidth=0.6,\n",
    "    zorder=0\n",
    ")\n",
    "\n",
    "\n",
    "ax.plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    'r--', linewidth=1.5\n",
    ")\n",
    "\n",
    "\n",
    "x_min, x_max = y_test.min(), y_test.max()\n",
    "ax.set_xlim([x_min - 40, x_max + 40])\n",
    "ax.set_ylim([x_min - 40, x_max + 40])\n",
    "ax.set_xlabel('Actual Curie Temperature (K)', labelpad=10)\n",
    "ax.set_ylabel('Predicted Curie Temperature (K)', labelpad=10)\n",
    "ax.set_title('XGBoost Ensemble with Stratified Undersampling', pad=20)\n",
    "\n",
    "\n",
    "stats_text = f\"R² = {r2:.2f}\\nMAE = {mae:.0f} K\\nRMSE = {rmse:.0f} K\"\n",
    "ax.text(\n",
    "    0.05, 0.95, stats_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=14,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.5')\n",
    ")\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax, pad=0.03)\n",
    "cbar.set_label('Prediction Std Deviation (K)', size=14)\n",
    "\n",
    "# Save plot\n",
    "plt.tight_layout(pad=2.5)\n",
    "#plt.savefig(\"filepath_to_save_plot\", dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce3ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c612fd36",
   "metadata": {},
   "source": [
    "##plot the histogram for the error bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15209efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "\n",
    "abs_errors = np.abs(y_test - y_pred)\n",
    "mae = np.mean(abs_errors)\n",
    "mae_rounded = round(mae)\n",
    "\n",
    "\n",
    "loc, scale = expon.fit(abs_errors)\n",
    "\n",
    "\n",
    "cutoff = 500\n",
    "filtered_errors = abs_errors[abs_errors <= cutoff]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "counts, bins, _ = plt.hist(\n",
    "    filtered_errors, bins=50, density=True, \n",
    "    alpha=0.85, color='royalblue', label='Histogram'\n",
    ")\n",
    "\n",
    "\n",
    "x_vals = np.linspace(0, cutoff, 500)\n",
    "plt.plot(\n",
    "    x_vals, expon.pdf(x_vals, loc, scale), \n",
    "    'k-', lw=2, label='Fitted Exponential Distribution'\n",
    ")\n",
    "\n",
    "\n",
    "plt.axvline(\n",
    "    mae, color='red', linestyle='--', lw=2,\n",
    "    label=f'Mean Absolute Error: {mae_rounded} K'\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel('Absolute Error (K)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Absolute Prediction Errors')\n",
    "plt.xlim(0, cutoff)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"filepath_to_save_plot\", dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766746b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b46803d",
   "metadata": {},
   "source": [
    "## feature importance plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"xtick.major.width\": 1.1,\n",
    "    \"ytick.major.width\": 1.1,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\"\n",
    "})\n",
    "\n",
    "\n",
    "#   Gain-based feature importances from XGBoost ensemble\n",
    "\n",
    "all_imps = []\n",
    "for m in models:  \n",
    "    booster  = m.get_booster()\n",
    "    imp_dict = booster.get_score(importance_type='gain')  \n",
    "    imp_arr  = np.array([imp_dict.get(feat, 0.0) for feat in FEATURES])\n",
    "    all_imps.append(imp_arr)\n",
    "\n",
    "all_imps = np.vstack(all_imps)            \n",
    "mean_imp = all_imps.mean(axis=0)\n",
    "std_imp  = all_imps.std(axis=0)\n",
    "\n",
    "\n",
    "# Build a DataFrame and pick Top-20 features\n",
    "\n",
    "imp_df = (\n",
    "    pd.DataFrame({\n",
    "        'feature': FEATURES,\n",
    "        'mean_importance': mean_imp,\n",
    "        'std_importance': std_imp\n",
    "    })\n",
    "    .sort_values('mean_importance', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "TOP_N = 20\n",
    "top20 = imp_df.head(TOP_N).copy()\n",
    "\n",
    "\n",
    "# Names mapping (for plot only)\n",
    "\n",
    "pretty_names = {\n",
    "    'avg_magnetic_moment'      : 'Avg Magnetic Moment',\n",
    "    'Average_Weight'           : 'Avg Atomic Weight',\n",
    "    'Magnetic_proportion'      : 'Prop. of High Curie Elements',\n",
    "    'Average_Electronegativity': 'Avg Electronegativity',\n",
    "    'Avg_Atomic_Number'        : 'Avg Atomic Number',\n",
    "    'Entropy'                  : 'Avg Entropy',\n",
    "    'Rare_Earth_proportion'    : 'Proportion of RE Elements',\n",
    "    'average_group'            : 'Avg Group',\n",
    "    'average_period'           : 'Avg Period',\n",
    "}\n",
    "top20['label'] = top20['feature'].map(pretty_names).fillna(top20['feature'])\n",
    "\n",
    "\n",
    "#  Plot Top-20 Feature Importances (uniform with RF plot)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(\n",
    "    top20['label'][::-1],                \n",
    "    top20['mean_importance'][::-1],\n",
    "    xerr=top20['std_importance'][::-1],\n",
    "    color='royalblue',\n",
    "    edgecolor='black',\n",
    "    capsize=3,\n",
    "    zorder=3   \n",
    ")\n",
    "\n",
    "\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.4, zorder=0)\n",
    "\n",
    "plt.xlabel('Average gain importance\\n(Ensemble Mean ± 1 SD)', fontsize=13)\n",
    "plt.title('Top-20 Feature Importances — XGBoost Ensemble', fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"filepath\", dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
