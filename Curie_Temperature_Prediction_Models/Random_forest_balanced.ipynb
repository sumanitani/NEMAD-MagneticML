{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef833c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "csv_path = (\"filepath\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "FEATURES = [\n",
    "    'Ti','Er','Rb','Ho','Pu','Cs','Zr','S','Tb','N','Ge','Pd','Mg','Re','La','K',\n",
    "    'Hf','P','Br','Ag','Os','F','Sc','Cm','Mo','In','Cl','Hg','Se','Tm','Ir','W',\n",
    "    'Th','H','Te','Np','Zn','Li','Gd','Ni','Co','Bi','I','Pr','Cd','Nb','Pa','Pt',\n",
    "    'Si','U','V','Sb','Mn','Na','Ce','Yb','Ta','Nd','Rh','O','Au','Sr','Eu','C',\n",
    "    'Pb','Ca','Cr','Cu','Ga','Fe','Y','As','Sn','B','Ba','Dy','Be','Sm','Lu','Al',\n",
    "    'Tl','Ru','Avg_Atomic_Number','Average_Weight','Average_Electronegativity',\n",
    "    'Magnetic_proportion','Entropy','average_period','avg_magnetic_moment',\n",
    "    'average_group','Rare_Earth_proportion'\n",
    "]\n",
    "TARGET = 'Mean_TC_K'\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "\n",
    "bins = pd.cut(\n",
    "    y,\n",
    "    bins=[-np.inf, 60, 165, 270, 325, 500, 665, np.inf],\n",
    "    labels=False\n",
    ")\n",
    "\n",
    "#  Stratified train/test split\n",
    "X_train, X_test, y_train, y_test, bins_train, _ = train_test_split(\n",
    "    X, y, bins,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=bins\n",
    ")\n",
    "\n",
    "\n",
    "train_bin_counts = pd.Series(bins_train).value_counts().sort_index()\n",
    "print(\"Train bin counts:\\n\", train_bin_counts)\n",
    "minority_size = train_bin_counts.min()\n",
    "print(f\"\\nSmallest bin size = {minority_size} samples\\n\")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 500, 1000],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "r2_scores, mae_scores = [], []\n",
    "M_per_fold = 5\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(cv.split(X_train, bins_train), 1):\n",
    "    Xt, Xv = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "    yt, yv = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "    bins_tr = bins_train.iloc[tr_idx]\n",
    "\n",
    "    counts = pd.Series(bins_tr).value_counts()\n",
    "    min_count = counts.min()\n",
    "\n",
    "    val_preds = np.zeros((len(Xv), M_per_fold))\n",
    "    for i in range(M_per_fold):\n",
    "        idxs = []\n",
    "        rng = np.random.RandomState(100 + i)\n",
    "        for b, cnt in counts.items():\n",
    "            b_idxs = np.where(bins_tr == b)[0]\n",
    "            sel = b_idxs if len(b_idxs) <= min_count else rng.choice(b_idxs, size=min_count, replace=False)\n",
    "            idxs.append(sel)\n",
    "        idxs = np.concatenate(idxs)\n",
    "\n",
    "        m = RandomForestRegressor(random_state=100 + i, n_jobs=1, **best_params)\n",
    "        m.fit(Xt.iloc[idxs], yt.iloc[idxs])\n",
    "        val_preds[:, i] = m.predict(Xv)\n",
    "\n",
    "    yv_pred = val_preds.mean(axis=1)\n",
    "    r2_scores.append(r2_score(yv, yv_pred))\n",
    "    mae_scores.append(mean_absolute_error(yv, yv_pred))\n",
    "    print(f\" Fold {fold}: R²={r2_scores[-1]:.3f}, MAE={mae_scores[-1]:.1f} K\")\n",
    "\n",
    "print(\"\\nValidation performance (mean ± std):\")\n",
    "print(f\" R²  : {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
    "print(f\" MAE : {np.mean(mae_scores):.1f} ± {np.std(mae_scores):.1f} K\\n\")\n",
    "\n",
    "\n",
    "M_final = 20\n",
    "models = []\n",
    "for i in range(M_final):\n",
    "    idxs = []\n",
    "    rng = np.random.RandomState(200 + i)\n",
    "    for b, cnt in train_bin_counts.items():\n",
    "        b_idxs = np.where(bins_train == b)[0]\n",
    "        sel = b_idxs if len(b_idxs) <= minority_size else rng.choice(b_idxs, size=minority_size, replace=False)\n",
    "        idxs.append(sel)\n",
    "    idxs = np.concatenate(idxs)\n",
    "\n",
    "    m = RandomForestRegressor(random_state=200 + i, n_jobs=1, **best_params)\n",
    "    m.fit(X_train.iloc[idxs], y_train.iloc[idxs])\n",
    "    models.append(m)\n",
    "\n",
    "\n",
    "all_preds = np.column_stack([m.predict(X_test) for m in models])\n",
    "y_pred = all_preds.mean(axis=1)\n",
    "y_std  = all_preds.std(axis=1)\n",
    "\n",
    "\n",
    "print(\"Test performance:\")\n",
    "print(f\" R²  : {r2_score(y_test, y_pred):.3f}\")\n",
    "print(f\" MAE : {mean_absolute_error(y_test, y_pred):.1f} K\")\n",
    "print(f\" RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.1f} K\\n\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.errorbar(\n",
    "    y_test, y_pred,\n",
    "    yerr=y_std,\n",
    "    fmt='o',\n",
    "    ecolor='lightgray',\n",
    "    capsize=2,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    'r--', lw=1\n",
    ")\n",
    "plt.xlabel('Actual Mean Curie Temperature (K)')\n",
    "plt.ylabel('Predicted Mean Curie Temperature (K)')\n",
    "plt.title('Ensemble Predictions ±1σ on TEST')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot Predicted vs Actual Curie Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e528543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"xtick.major.width\": 1.1,\n",
    "    \"ytick.major.width\": 1.1,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\"\n",
    "})\n",
    "\n",
    "norm   = plt.Normalize(vmin=y_std.min(), vmax=y_std.max())\n",
    "colors = cm.turbo(norm(y_std))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.1, 6))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    y_test, y_pred,\n",
    "    c=y_std, cmap='turbo',\n",
    "    edgecolor='black', linewidth=0.25,\n",
    "    s=65, alpha=0.9\n",
    ")\n",
    "\n",
    "ax.errorbar(\n",
    "    y_test, y_pred,\n",
    "    yerr=y_std,\n",
    "    fmt='none',\n",
    "    ecolor='gray',\n",
    "    alpha=0.2,\n",
    "    capsize=2,\n",
    "    linewidth=0.6,\n",
    "    zorder=0\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    'r--', linewidth=1.5\n",
    ")\n",
    "\n",
    "x_min, x_max = y_test.min(), y_test.max()\n",
    "ax.set_xlim([x_min - 40, x_max + 40])\n",
    "ax.set_ylim([x_min - 40, x_max + 40])\n",
    "ax.set_xlabel('Actual Curie Temperature (K)', labelpad=10)\n",
    "ax.set_ylabel('Predicted Curie Temperature (K)', labelpad=10)\n",
    "ax.set_title('RF with Stratified Undersampling', pad=20)\n",
    "\n",
    "stats_text = f\"R² = {r2:.2f}\\nMAE = {mae:.0f} K\\nRMSE = {rmse:.0f} K\"\n",
    "ax.text(\n",
    "    0.05, 0.95, stats_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=14,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.5')\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax, pad=0.03)\n",
    "cbar.set_label('Prediction Std Deviation (K)', size=14)\n",
    "\n",
    "plt.tight_layout(pad=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6af5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196b3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code of the error bar histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ccdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "abs_errors = np.abs(y_test - y_pred)\n",
    "mae = np.mean(abs_errors)\n",
    "mae_rounded = round(mae)\n",
    "\n",
    "loc, scale = expon.fit(abs_errors)\n",
    "\n",
    "cutoff = 600\n",
    "filtered_errors = abs_errors[abs_errors <= cutoff]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "counts, bins, _ = plt.hist(\n",
    "    filtered_errors, bins=30, density=True, \n",
    "    alpha=0.85, color='royalblue', label='Histogram'\n",
    ")\n",
    "\n",
    "x_vals = np.linspace(0, cutoff, 500)\n",
    "plt.plot(\n",
    "    x_vals, expon.pdf(x_vals, loc, scale),\n",
    "    'k-', lw=2, label='Fitted Exponential Distribution'\n",
    ")\n",
    "\n",
    "plt.axvline(\n",
    "    mae, color='red', linestyle='--', lw=2,\n",
    "    label=f'Mean Absolute Error: {mae_rounded} K'\n",
    ")\n",
    "\n",
    "plt.xlabel('Absolute Error (K)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Absolute Prediction Errors (RF)')\n",
    "plt.xlim(0, cutoff)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d41ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\":     20,\n",
    "    \"axes.labelsize\":     18,\n",
    "    \"xtick.labelsize\":    16,\n",
    "    \"ytick.labelsize\":    16,\n",
    "    \"font.family\":       \"sans-serif\",\n",
    "    \"font.sans-serif\":   \"Helvetica\",\n",
    "    \"legend.fontsize\":    13,\n",
    "    \"axes.linewidth\":     1.2,\n",
    "    \"xtick.major.width\":  1.1,\n",
    "    \"ytick.major.width\":  1.1,\n",
    "    \"grid.alpha\":         0.3,\n",
    "    \"grid.linestyle\":    \"--\"\n",
    "})\n",
    "\n",
    "all_imps = [m.feature_importances_ for m in models]\n",
    "all_imps = np.vstack(all_imps)\n",
    "mean_imp = all_imps.mean(axis=0)\n",
    "std_imp  = all_imps.std(axis=0)\n",
    "\n",
    "imp_df = (\n",
    "    pd.DataFrame({\n",
    "        'feature'         : FEATURES,\n",
    "        'mean_importance' : mean_imp,\n",
    "        'std_importance'  : std_imp\n",
    "    })\n",
    "    .sort_values('mean_importance', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "TOP_N = 20\n",
    "top20 = imp_df.head(TOP_N).copy()\n",
    "\n",
    "pretty_names = {\n",
    "    'avg_magnetic_moment'      : 'Avg Magnetic Moment',\n",
    "    'Average_Weight'           : 'Avg Atomic Weight',\n",
    "    'Magnetic_proportion'      : 'Prop. of High Curie Elements',\n",
    "    'Average_Electronegativity': 'Avg Electronegativity',\n",
    "    'Avg_Atomic_Number'        : 'Avg Atomic Number',\n",
    "    'Entropy'                  : 'Avg Entropy',\n",
    "    'Rare_Earth_proportion'    : 'Prop. of RE Elements',\n",
    "    'average_group'            : 'Avg Group',\n",
    "    'average_period'           : 'Avg Period',\n",
    "}\n",
    "top20['label'] = top20['feature'].map(pretty_names).fillna(top20['feature'])\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(\n",
    "    top20['label'][::-1],\n",
    "    top20['mean_importance'][::-1],\n",
    "    xerr=top20['std_importance'][::-1],\n",
    "    color='royalblue',\n",
    "    edgecolor='black',\n",
    "    zorder=3,\n",
    "    capsize=3\n",
    ")\n",
    "plt.xlabel('Average Importance\\n(Ensemble Mean ± 1 SD)', fontsize=13)\n",
    "plt.title('Top-20 Feature Importances — Random Forest Ensemble', fontsize=15)\n",
    "plt.grid(axis='both', linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e022336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
