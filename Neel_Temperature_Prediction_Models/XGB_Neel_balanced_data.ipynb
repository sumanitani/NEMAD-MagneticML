{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##XGBoost for Neel Temperature prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5284558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "csv_path = ( \"filepath\"\n",
    ")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "FEATURES = [\n",
    "    'Nb', 'Ca', 'Tm', 'S', 'Sb', 'U', 'Mo', 'Sm', 'I', 'Ag', 'Ir', 'Pu', 'Hf', 'Sc', 'Br', 'Ni',\n",
    "    'Co', 'Zr', 'W', 'Tb', 'Mn', 'Fe', 'Cr', 'Pb', 'Si', 'Ba', 'Nd', 'Cs', 'N', 'Lu', 'Se', 'Pt',\n",
    "    'Pr', 'Au', 'La', 'Re', 'Zn', 'Os', 'Ti', 'Be', 'Ce', 'Cu', 'In', 'Rb', 'Tl', 'F', 'Dy', 'V',\n",
    "    'Er', 'Ho', 'Pd', 'H', 'B', 'C', 'Cl', 'P', 'Hg', 'Sr', 'Gd', 'Ga', 'K', 'Te', 'Th', 'Yb',\n",
    "    'Al', 'Tc', 'Np', 'Ta', 'Rh', 'Ru', 'As', 'Cd', 'Li', 'Sn', 'Ge', 'Mg', 'Eu', 'Na', 'O', 'Y', 'Bi',\n",
    "    'Avg_Atomic_Number', 'Average_Weight', 'Average_Electronegativity',\n",
    "    'Magnetic_proportion', 'Entropy', 'average_period', 'avg_magnetic_moment',\n",
    "    'average_group', 'Rare_Earth_proportion'\n",
    "]\n",
    "TARGET = 'Mean_TN_K'\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "bins = pd.cut(\n",
    "    y,\n",
    "    bins=[-np.inf, 8, 20, 50, 125, 300, np.inf],\n",
    "    labels=False\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test, bins_train, bins_test = train_test_split(\n",
    "    X, y, bins,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=bins\n",
    ")\n",
    "\n",
    "train_bin_counts = pd.Series(bins_train).value_counts().sort_index()\n",
    "minority_bin_size = train_bin_counts.min()\n",
    "print(\"Train bin counts:\\n\", train_bin_counts)\n",
    "print(f\"\\nSmallest bin size = {minority_bin_size} samples\\n\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [800, 1200, 1500],\n",
    "    'max_depth': [6, 12, 18],\n",
    "    'learning_rate': [0.01, 0.08, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "xgb_estimator = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_estimator,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "r2_scores, mae_scores = [], []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(cv.split(X_train, bins_train), 1):\n",
    "    Xt, Xv = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "    yt, yv = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "    bins_tr = bins_train.iloc[tr_idx]\n",
    "\n",
    "    fold_counts = pd.Series(bins_tr).value_counts()\n",
    "    fold_min = fold_counts.min()\n",
    "\n",
    "    M = 2\n",
    "    val_preds = np.zeros((len(Xv), M))\n",
    "\n",
    "    for i in range(M):\n",
    "        idxs = []\n",
    "        rng = np.random.RandomState(100 + i)\n",
    "        for b in fold_counts.index:\n",
    "            b_idxs = np.where(bins_tr == b)[0]\n",
    "            sel = b_idxs if len(b_idxs) <= fold_min else rng.choice(b_idxs, size=fold_min, replace=False)\n",
    "            idxs.append(sel)\n",
    "        idxs = np.concatenate(idxs)\n",
    "\n",
    "        mb = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=100 + i,\n",
    "            n_jobs=1,\n",
    "            **best_params\n",
    "        )\n",
    "        mb.fit(Xt.iloc[idxs], yt.iloc[idxs])\n",
    "        val_preds[:, i] = mb.predict(Xv)\n",
    "\n",
    "    yv_pred = val_preds.mean(axis=1)\n",
    "    r2_scores.append(r2_score(yv, yv_pred))\n",
    "    mae_scores.append(mean_absolute_error(yv, yv_pred))\n",
    "    print(f\" Fold {fold}: R² = {r2_scores[-1]:.3f}, MAE = {mae_scores[-1]:.1f}\")\n",
    "\n",
    "print(\"\\nValidation performance (mean ± std):\")\n",
    "print(f\" R²  : {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
    "print(f\" MAE : {np.mean(mae_scores):.1f} ± {np.std(mae_scores):.1f}\")\n",
    "\n",
    "M_final = 30\n",
    "models = []\n",
    "\n",
    "for i in range(M_final):\n",
    "    idxs = []\n",
    "    rng = np.random.RandomState(200 + i)\n",
    "    for b in train_bin_counts.index:\n",
    "        b_idxs = np.where(bins_train == b)[0]\n",
    "        sel = b_idxs if len(b_idxs) <= minority_bin_size else rng.choice(b_idxs, size=minority_bin_size, replace=False)\n",
    "        idxs.append(sel)\n",
    "    idxs = np.concatenate(idxs)\n",
    "\n",
    "    m = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=200 + i,\n",
    "        n_jobs=1,\n",
    "        **best_params\n",
    "    )\n",
    "    m.fit(X_train.iloc[idxs], y_train.iloc[idxs])\n",
    "    models.append(m)\n",
    "\n",
    "all_preds = np.column_stack([m.predict(X_test) for m in models])\n",
    "y_pred = all_preds.mean(axis=1)\n",
    "y_std  = all_preds.std(axis=1)\n",
    "\n",
    "print(\"\\nTest performance:\")\n",
    "print(f\" R²  : {r2_score(y_test, y_pred):.3f}\")\n",
    "print(f\" MAE : {mean_absolute_error(y_test, y_pred):.1f} K\")\n",
    "print(f\" RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.1f} K\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.errorbar(\n",
    "    y_test, y_pred,\n",
    "    yerr=y_std,\n",
    "    fmt='o',\n",
    "    ecolor='lightgray',\n",
    "    capsize=2,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    'r--', lw=1.5\n",
    ")\n",
    "plt.xlabel('Actual Mean Néel Temperature (K)')\n",
    "plt.ylabel('Predicted Mean Néel Temperature (K)')\n",
    "plt.title('XGBoost Ensemble Predictions ±1σ on TEST — Néel Temperature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d1689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7211d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predicted vs actual Neel temperature plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"xtick.major.width\": 1.1,\n",
    "    \"ytick.major.width\": 1.1,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\"\n",
    "})\n",
    "\n",
    "norm = plt.Normalize(vmin=y_std.min(), vmax=y_std.max())\n",
    "colors = cm.turbo(norm(y_std))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.1, 6))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    y_test, y_pred,\n",
    "    c=y_std, cmap='turbo',\n",
    "    edgecolor='black', linewidth=0.25,\n",
    "    s=65, alpha=0.9\n",
    ")\n",
    "\n",
    "ax.errorbar(\n",
    "    y_test, y_pred,\n",
    "    yerr=y_std,\n",
    "    fmt='none',\n",
    "    ecolor='gray',\n",
    "    alpha=0.2,\n",
    "    capsize=2,\n",
    "    linewidth=0.6,\n",
    "    zorder=0\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    'r--', linewidth=1.5\n",
    ")\n",
    "\n",
    "x_min, x_max = y_test.min(), y_test.max()\n",
    "ax.set_xlim([x_min - 40, x_max + 40])\n",
    "ax.set_ylim([x_min - 40, x_max + 40])\n",
    "ax.set_xlabel('Actual Néel Temperature (K)', labelpad=10)\n",
    "ax.set_ylabel('Predicted Néel Temperature (K)', labelpad=10)\n",
    "ax.set_title('XGBoost with Stratified Undersampling', pad=20)\n",
    "\n",
    "stats_text = f\"R² = {r2:.2f}\\nMAE = {mae:.0f} K\\nRMSE = {rmse:.0f} K\"\n",
    "ax.text(\n",
    "    0.05, 0.95, stats_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=14,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.5')\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax, pad=0.03)\n",
    "cbar.set_label('Prediction Std Deviation (K)', size=14)\n",
    "\n",
    "plt.tight_layout(pad=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415cbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65229834",
   "metadata": {},
   "outputs": [],
   "source": [
    "### mean absolute error distribution plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7089bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "abs_errors = np.abs(y_test - y_pred)\n",
    "mae = np.mean(abs_errors)\n",
    "mae_rounded = round(mae)\n",
    "\n",
    "loc, scale = expon.fit(abs_errors)\n",
    "\n",
    "cutoff = 250\n",
    "filtered_errors = abs_errors[abs_errors <= cutoff]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "counts, bins, _ = plt.hist(\n",
    "    filtered_errors, bins=50, density=True, \n",
    "    alpha=0.85, color='royalblue', label='Histogram'\n",
    ")\n",
    "\n",
    "x_vals = np.linspace(0, cutoff, 500)\n",
    "plt.plot(\n",
    "    x_vals, expon.pdf(x_vals, loc, scale), \n",
    "    'k-', lw=2, label='Fitted Exponential Distribution'\n",
    ")\n",
    "\n",
    "plt.axvline(\n",
    "    mae, color='red', linestyle='--', lw=2,\n",
    "    label=f'Mean Absolute Error: {mae_rounded} K'\n",
    ")\n",
    "\n",
    "plt.xlabel('Absolute Error (K)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Absolute Error')\n",
    "plt.xlim(0, cutoff)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62fe7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b07699",
   "metadata": {},
   "outputs": [],
   "source": [
    "###feature importance plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dcf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import xgboost as xgb\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"xtick.major.width\": 1.1,\n",
    "    \"ytick.major.width\": 1.1,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\"\n",
    "})\n",
    "\n",
    "all_imps = []\n",
    "for m in models:\n",
    "    booster  = m.get_booster()\n",
    "    imp_dict = booster.get_score(importance_type='gain')  \n",
    "    imp_arr  = np.array([imp_dict.get(feat, 0.0) for feat in FEATURES])\n",
    "    all_imps.append(imp_arr)\n",
    "\n",
    "all_imps = np.vstack(all_imps)            \n",
    "mean_imp = all_imps.mean(axis=0)\n",
    "std_imp  = all_imps.std(axis=0)\n",
    "\n",
    "imp_df = (\n",
    "    pd.DataFrame({\n",
    "        'feature': FEATURES,\n",
    "        'mean_importance': mean_imp,\n",
    "        'std_importance': std_imp\n",
    "    })\n",
    "    .sort_values('mean_importance', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "TOP_N = 20\n",
    "top20 = imp_df.head(TOP_N).copy()\n",
    "\n",
    "pretty_names = {\n",
    "    'avg_magnetic_moment': 'Avg Magnetic Moment',\n",
    "    'Average_Weight': 'Avg Atomic Weight',\n",
    "    'Magnetic_proportion': 'Prop. of High Néel Elements',\n",
    "    'Average_Electronegativity': 'Avg Electronegativity',\n",
    "    'Avg_Atomic_Number': 'Avg Atomic Number',\n",
    "    'Entropy': 'Avg Entropy',\n",
    "    'Rare_Earth_proportion': 'Proportion of RE Elements',\n",
    "    'average_group': 'Avg Group',\n",
    "    'average_period': 'Avg Period',\n",
    "}\n",
    "top20['label'] = top20['feature'].map(pretty_names).fillna(top20['feature'])\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(\n",
    "    top20['label'][::-1],\n",
    "    top20['mean_importance'][::-1],\n",
    "    xerr=top20['std_importance'][::-1],\n",
    "    color='royalblue',\n",
    "    edgecolor='black',\n",
    "    capsize=3,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "plt.xlabel('Average gain importance\\n(Ensemble Mean ± 1 SD)', fontsize=13)\n",
    "plt.title('Top-20 Feature Importances — XGBoost Ensemble', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452fb15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
