{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e7fa0",
   "metadata": {},
   "source": [
    "##Ensemble Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784021ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "csv_path = (\n",
    "    \"/Users/Suman/Desktop/Python_file/data/LLM/\"\n",
    "    \"Paper_download_with_table/csv_chunks/result/cleaned_version/\"\n",
    "    \"Clean_for_model/Model_for_all_data/\"\n",
    "    \"model_set/Reviewers_comment_address/Curie_All_experimental17.csv\"\n",
    ")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "FEATURES = [\n",
    "    'Ti','Er','Rb','Ho','Pu','Cs','Zr','S','Tb','N','Ge','Pd','Mg','Re','La','K',\n",
    "    'Hf','P','Br','Ag','Os','F','Sc','Cm','Mo','In','Cl','Hg','Se','Tm','Ir','W',\n",
    "    'Th','H','Te','Np','Zn','Li','Gd','Ni','Co','Bi','I','Pr','Cd','Nb','Pa','Pt',\n",
    "    'Si','U','V','Sb','Mn','Na','Ce','Yb','Ta','Nd','Rh','O','Au','Sr','Eu','C',\n",
    "    'Pb','Ca','Cr','Cu','Ga','Fe','Y','As','Sn','B','Ba','Dy','Be','Sm','Lu','Al',\n",
    "    'Tl','Ru','Avg_Atomic_Number','Average_Weight','Average_Electronegativity',\n",
    "    'Magnetic_proportion','Entropy','average_period','avg_magnetic_moment',\n",
    "    'average_group','Rare_Earth_proportion'\n",
    "]\n",
    "TARGET = 'Mean_TC_K'\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "\n",
    "bins = pd.cut(\n",
    "    y,\n",
    "    bins=[-np.inf, 60, 165, 270, 325, 500, 665, np.inf],\n",
    "    labels=False\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Stratified train/test split\n",
    "# ------------------------------------------------\n",
    "X_train, X_test, y_train, y_test, bins_train, bins_test = train_test_split(\n",
    "    X, y, bins,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=bins\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "#  Standardize\n",
    "# ------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ------------------------------------------------\n",
    "#  Look at bin counts\n",
    "# ------------------------------------------------\n",
    "train_bin_counts = pd.Series(bins_train).value_counts().sort_index()\n",
    "minority_bin_size = train_bin_counts.min()\n",
    "print(\"Train bin counts:\\n\", train_bin_counts)\n",
    "print(f\"Smallest bin size = {minority_bin_size}\\n\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define Neural Network model builder\n",
    "# ------------------------------------------------\n",
    "def create_model(learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# ------------------------------------------------\n",
    "#  Hyperparameter tuning via GridSearchCV\n",
    "# ------------------------------------------------\n",
    "keras_reg = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2],\n",
    "    'batch_size': [32, 64,128],\n",
    "    'epochs': [100, 300, 500,800]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=keras_reg,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_result.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Use best hyperparameters \n",
    "BEST_LR     = best_params['learning_rate']\n",
    "BEST_BATCH  = best_params['batch_size']\n",
    "BEST_EPOCHS = best_params['epochs']\n",
    "\n",
    "# ------------------------------------------------\n",
    "#  Cross-validation on TRAINING set\n",
    "# ------------------------------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "r2_scores, mae_scores = [], []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(cv.split(X_train_scaled, bins_train), 1):\n",
    "    Xt, Xv = X_train_scaled[tr_idx], X_train_scaled[val_idx]\n",
    "    yt, yv = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "    bins_tr = bins_train.iloc[tr_idx]\n",
    "\n",
    "    fold_counts = pd.Series(bins_tr).value_counts()\n",
    "    fold_min = fold_counts.min()\n",
    "\n",
    "    M = 5\n",
    "    val_preds = np.zeros((len(Xv), M))\n",
    "\n",
    "    for i in range(M):\n",
    "        idxs = []\n",
    "        for b in fold_counts.index:\n",
    "            b_idxs = np.where(bins_tr == b)[0]\n",
    "            sel = b_idxs if len(b_idxs) <= fold_min else np.random.RandomState(100+i).choice(b_idxs, size=fold_min, replace=False)\n",
    "            idxs.append(sel)\n",
    "        idxs = np.concatenate(idxs)\n",
    "\n",
    "        model = create_model(learning_rate=BEST_LR)\n",
    "        model.fit(Xt[idxs], yt.iloc[idxs], epochs=BEST_EPOCHS, batch_size=BEST_BATCH, verbose=0)\n",
    "        val_preds[:, i] = model.predict(Xv).flatten()\n",
    "\n",
    "    yv_pred = val_preds.mean(axis=1)\n",
    "    r2_scores.append(r2_score(yv, yv_pred))\n",
    "    mae_scores.append(mean_absolute_error(yv, yv_pred))\n",
    "    print(f\" Fold {fold:>2}:   R²={r2_scores[-1]:.3f}, MAE={mae_scores[-1]:.1f}\")\n",
    "\n",
    "print(\"\\nValidation performance (mean ± std):\")\n",
    "print(f\" R²  : {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
    "print(f\" MAE : {np.mean(mae_scores):.1f} ± {np.std(mae_scores):.1f}\\n\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "#  Train final ensemble on FULL training set\n",
    "# ------------------------------------------------\n",
    "M_final = 30\n",
    "models = []\n",
    "\n",
    "for i in range(M_final):\n",
    "    idxs = []\n",
    "    for b in train_bin_counts.index:\n",
    "        b_idxs = np.where(bins_train == b)[0]\n",
    "        sel = b_idxs if len(b_idxs) <= minority_bin_size else np.random.RandomState(200+i).choice(b_idxs, size=minority_bin_size, replace=False)\n",
    "        idxs.append(sel)\n",
    "    idxs = np.concatenate(idxs)\n",
    "\n",
    "    model = create_model(learning_rate=BEST_LR)\n",
    "    model.fit(X_train_scaled[idxs], y_train.iloc[idxs], epochs=BEST_EPOCHS, batch_size=BEST_BATCH, verbose=0)\n",
    "    models.append(model)\n",
    "\n",
    "# ------------------------------------------------\n",
    "#  Predict on TEST\n",
    "# ------------------------------------------------\n",
    "all_preds = np.column_stack([m.predict(X_test_scaled).flatten() for m in models])\n",
    "y_pred_mean = all_preds.mean(axis=1)\n",
    "y_pred_std  = all_preds.std(axis=1)\n",
    "\n",
    "\n",
    "print(\" Test performance:\")\n",
    "print(\" R²  :\", r2_score(y_test, y_pred_mean))\n",
    "print(\" MAE :\", mean_absolute_error(y_test, y_pred_mean))\n",
    "print(\" RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_mean)))\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.errorbar(\n",
    "    y_test, y_pred_mean,\n",
    "    yerr=y_pred_std,\n",
    "    fmt='o',\n",
    "    ecolor='lightgray',\n",
    "    capsize=2,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    'r--', lw=1\n",
    ")\n",
    "plt.xlabel('Actual Mean Curie Temperature (K)')\n",
    "plt.ylabel('Predicted Mean Curie Temperature (K)')\n",
    "plt.title('Neural Network Ensemble Predictions ±1σ on TEST')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791ebbf",
   "metadata": {},
   "source": [
    "## Plot of predicted vs actual curie temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db593a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "r2   = r2_score(y_test, y_pred_mean)\n",
    "mae  = mean_absolute_error(y_test, y_pred_mean)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_mean))\n",
    "\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"xtick.major.width\": 1.1,\n",
    "    \"ytick.major.width\": 1.1,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\"\n",
    "})\n",
    "\n",
    "\n",
    "norm = plt.Normalize(vmin=y_pred_std.min(), vmax=y_pred_std.max())\n",
    "colors = cm.turbo(norm(y_pred_std)) \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.1, 6))  \n",
    "\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    y_test, y_pred_mean,\n",
    "    c=y_pred_std, cmap='turbo',\n",
    "    edgecolor='black', linewidth=0.25,\n",
    "    s=65, alpha=0.9\n",
    ")\n",
    "\n",
    "\n",
    "ax.errorbar(\n",
    "    y_test, y_pred_mean,\n",
    "    yerr=y_pred_std,\n",
    "    fmt='none',\n",
    "    ecolor='gray',\n",
    "    alpha=0.2,\n",
    "    capsize=2,\n",
    "    linewidth=0.6,\n",
    "    zorder=0\n",
    ")\n",
    "\n",
    "\n",
    "ax.plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    'r--', linewidth=1.5\n",
    ")\n",
    "\n",
    "\n",
    "x_min, x_max = y_test.min(), y_test.max()\n",
    "ax.set_xlim([x_min - 40, x_max + 40])\n",
    "ax.set_ylim([x_min - 40, x_max + 40])\n",
    "ax.set_xlabel('Actual Curie Temperature (K)', labelpad=10)\n",
    "ax.set_ylabel('Predicted Curie Temperature (K)', labelpad=10)\n",
    "ax.set_title('ENN with Stratified Undersampling', pad=20)\n",
    "\n",
    "\n",
    "stats_text = f\"R² = {r2:.2f}\\nMAE = {mae:.0f} K\\nRMSE = {rmse:.0f} K\"\n",
    "ax.text(\n",
    "    0.05, 0.95, stats_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=14,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.5')\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax, pad=0.03)\n",
    "cbar.set_label('Prediction Std Deviation (K)', size=14)\n",
    "plt.tight_layout(pad=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026ee0c",
   "metadata": {},
   "source": [
    "##distribution of the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c997a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "abs_errors = np.abs(y_test - y_pred_mean)\n",
    "mae = np.mean(abs_errors)\n",
    "mae_rounded = round(mae)\n",
    "\n",
    "loc, scale = expon.fit(abs_errors)\n",
    "\n",
    "cutoff = 600\n",
    "filtered_errors = abs_errors[abs_errors <= cutoff]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "counts, bins, _ = plt.hist(\n",
    "    filtered_errors, bins=30, density=True,\n",
    "    alpha=0.85, color='royalblue', label='Histogram'\n",
    ")\n",
    "\n",
    "x_vals = np.linspace(0, cutoff, 500)\n",
    "plt.plot(\n",
    "    x_vals, expon.pdf(x_vals, loc, scale),\n",
    "    'k-', lw=2, label='Fitted Exponential Distribution'\n",
    ")\n",
    "\n",
    "plt.axvline(\n",
    "    mae, color='red', linestyle='--', lw=2,\n",
    "    label=f'Mean Absolute Error: {mae_rounded} K'\n",
    ")\n",
    "\n",
    "plt.xlabel('Absolute Error (K)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Absolute Prediction Errors (ENN)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.xlim(0, cutoff)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036da32f",
   "metadata": {},
   "source": [
    "##Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"axes.titlesize\": 20,\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": \"Helvetica\",\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"xtick.major.width\": 1.1,\n",
    "    \"ytick.major.width\": 1.1,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \"--\"\n",
    "})\n",
    "\n",
    "n_ensembles = len(models)\n",
    "importances_mean = np.zeros(len(FEATURES))\n",
    "importances_std = np.zeros(len(FEATURES))\n",
    "\n",
    "for model in models:\n",
    "    results = permutation_importance(\n",
    "        model,\n",
    "        X_test_scaled,\n",
    "        y_test,\n",
    "        n_repeats=30,\n",
    "        random_state=42,\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    importances_mean += results.importances_mean\n",
    "    importances_std += results.importances_std\n",
    "\n",
    "importances_mean /= n_ensembles\n",
    "importances_std /= n_ensembles\n",
    "\n",
    "imp_df = (\n",
    "    pd.DataFrame({\n",
    "        'feature': FEATURES,\n",
    "        'mean_importance': importances_mean,\n",
    "        'std_importance': importances_std\n",
    "    })\n",
    "    .sort_values('mean_importance', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "TOP_N = 20\n",
    "top20 = imp_df.head(TOP_N).copy()\n",
    "\n",
    "pretty_names = {\n",
    "    'avg_magnetic_moment': 'Avg Magnetic Moment',\n",
    "    'Average_Weight': 'Avg Atomic Weight',\n",
    "    'Magnetic_proportion': 'Prop. of High Curie Elements',\n",
    "    'Average_Electronegativity': 'Avg Electronegativity',\n",
    "    'Avg_Atomic_Number': 'Avg Atomic Number',\n",
    "    'Entropy': 'Avg Entropy',\n",
    "    'Rare_Earth_proportion': 'Proportion of RE Elements',\n",
    "    'average_group': 'Avg Group',\n",
    "    'average_period': 'Avg Period',\n",
    "}\n",
    "\n",
    "top20['label'] = top20['feature'].map(pretty_names).fillna(top20['feature'])\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(\n",
    "    top20['label'][::-1],\n",
    "    top20['mean_importance'][::-1],\n",
    "    xerr=top20['std_importance'][::-1],\n",
    "    color='royalblue',\n",
    "    edgecolor='black',\n",
    "    capsize=3,\n",
    "    zorder=3\n",
    ")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.4, zorder=0)\n",
    "plt.xlabel('Average permutation importance\\n(Ensemble Mean ± 1 SD)', fontsize=13)\n",
    "plt.title('Top-20 Permutation Importances — Neural Network Ensemble', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1a3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a657f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptextractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
